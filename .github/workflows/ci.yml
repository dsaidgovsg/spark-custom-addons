name: CI

on:
  push:
    branches:
    - master
  pull_request:
    branches:
    - master

jobs:
  build:
    strategy:
      matrix:
        version:
        - dist:         "debian"
          spark:        "2.4.4"
          scala:        "2.11"
          hadoop:       "3.1.0"
          python:       "3.5"
          with_hive:    "true"
          with_pyspark: "true"
        - dist:         "debian"
          spark:        "2.4.4"
          scala:        "2.11"
          hadoop:       "3.1.0"
          python:       "3.6"
          with_hive:    "true"
          with_pyspark: "true"
        - dist:         "debian"
          spark:        "2.4.4"
          scala:        "2.11"
          hadoop:       "3.1.0"
          python:       "3.7"
          with_hive:    "true"
          with_pyspark: "true"
        - dist:         "debian"
          spark:        "2.4.4"
          scala:        "2.12"
          hadoop:       "3.1.0"
          python:       "3.5"
          with_hive:    "true"
          with_pyspark: "true"
        - dist:         "debian"
          spark:        "2.4.4"
          scala:        "2.12"
          hadoop:       "3.1.0"
          python:       "3.6"
          with_hive:    "true"
          with_pyspark: "true"
        - dist:         "debian"
          spark:        "2.4.4"
          scala:        "2.12"
          hadoop:       "3.1.0"
          python:       "3.7"
          with_hive:    "true"
          with_pyspark: "true"
        - dist:         "debian"
          spark:        "2.3.4"
          scala:        "2.11"
          hadoop:       "2.7.3"
          python:       "3.5"
          with_hive:    "true"
          with_pyspark: "true"
        - dist:         "debian"
          spark:        "2.3.4"
          scala:        "2.11"
          hadoop:       "2.7.3"
          python:       "3.6"
          with_hive:    "true"
          with_pyspark: "true"
        - dist:         "debian"
          spark:        "2.3.4"
          scala:        "2.11"
          hadoop:       "2.7.3"
          python:       "3.7"
          with_hive:    "true"
          with_pyspark: "true"
        - dist:         "debian"
          spark:        "2.3.4"
          scala:        "2.12"
          hadoop:       "2.7.3"
          python:       "3.5"
          with_hive:    "true"
          with_pyspark: "true"
        - dist:         "debian"
          spark:        "2.3.4"
          scala:        "2.12"
          hadoop:       "2.7.3"
          python:       "3.6"
          with_hive:    "true"
          with_pyspark: "true"
        - dist:         "debian"
          spark:        "2.3.4"
          scala:        "2.12"
          hadoop:       "2.7.3"
          python:       "3.7"
          with_hive:    "true"
          with_pyspark: "true"
        - dist:         "alpine"
          spark:        "2.4.4"
          scala:        "2.11"
          hadoop:       "3.1.0"
          python:       "3.5"
          with_hive:    "true"
          with_pyspark: "true"
        - dist:         "alpine"
          spark:        "2.4.4"
          scala:        "2.11"
          hadoop:       "3.1.0"
          python:       "3.6"
          with_hive:    "true"
          with_pyspark: "true"
        - dist:         "alpine"
          spark:        "2.4.4"
          scala:        "2.11"
          hadoop:       "3.1.0"
          python:       "3.7"
          with_hive:    "true"
          with_pyspark: "true"
        - dist:         "alpine"
          spark:        "2.4.4"
          scala:        "2.12"
          hadoop:       "3.1.0"
          python:       "3.5"
          with_hive:    "true"
          with_pyspark: "true"
        - dist:         "alpine"
          spark:        "2.4.4"
          scala:        "2.12"
          hadoop:       "3.1.0"
          python:       "3.6"
          with_hive:    "true"
          with_pyspark: "true"
        - dist:         "alpine"
          spark:        "2.4.4"
          scala:        "2.12"
          hadoop:       "3.1.0"
          python:       "3.7"
          with_hive:    "true"
          with_pyspark: "true"
        - dist:         "alpine"
          spark:        "2.3.4"
          scala:        "2.11"
          hadoop:       "2.7.3"
          python:       "3.5"
          with_hive:    "true"
          with_pyspark: "true"
        - dist:         "alpine"
          spark:        "2.3.4"
          scala:        "2.11"
          hadoop:       "2.7.3"
          python:       "3.6"
          with_hive:    "true"
          with_pyspark: "true"
        - dist:         "alpine"
          spark:        "2.3.4"
          scala:        "2.11"
          hadoop:       "2.7.3"
          python:       "3.7"
          with_hive:    "true"
          with_pyspark: "true"
        - dist:         "alpine"
          spark:        "2.3.4"
          scala:        "2.12"
          hadoop:       "2.7.3"
          python:       "3.5"
          with_hive:    "true"
          with_pyspark: "true"
        - dist:         "alpine"
          spark:        "2.3.4"
          scala:        "2.12"
          hadoop:       "2.7.3"
          python:       "3.6"
          with_hive:    "true"
          with_pyspark: "true"
        - dist:         "alpine"
          spark:        "2.3.4"
          scala:        "2.12"
          hadoop:       "2.7.3"
          python:       "3.7"
          with_hive:    "true"
          with_pyspark: "true"
    runs-on: ubuntu-latest
    env:
      IMAGE_NAME: spark-custom-addons
      SELF_VERSION: "v1"
      DIST: "${{ matrix.version.dist }}"
      SPARK_VERSION: "${{ matrix.version.spark }}"
      SCALA_VERSION: "${{ matrix.version.scala }}"
      HADOOP_VERSION: "${{ matrix.version.hadoop }}"
      PYTHON_VERSION: "${{ matrix.version.python }}"
      WITH_HIVE: "${{ matrix.version.with_hive }}"
      WITH_PYSPARK: "${{ matrix.version.with_pyspark }}"
    steps:
    - name: Checkout code
      uses: actions/checkout@v1
    - name: Install tera-cli
      run: |-
        wget https://github.com/guangie88/tera-cli/releases/download/v0.2.1/tera_linux_amd64 -O /tmp/tera
        chmod +x /tmp/tera
    - name: Check shell scripts
      run: |-
        shellcheck push-images.sh
        shellcheck templates/apply-vars.sh
    - name: Check differences between ci.yml and ci.yml.tmpl
      run: |-
        cp .github/workflows/ci.yml .github/workflows/ci.yml.backup
        TERA=/tmp/tera ./templates/apply-vars.sh
        if ! diff .github/workflows/ci.yml .github/workflows/ci.yml.backup; then echo "ci.yml.tmpl and ci.yml differs!" && exit 1; fi
    - name: Build Docker image
      run: |-
        HIVE_TAG_SUFFIX="$(if [ "${WITH_HIVE}" = "true" ]; then echo _hive; fi)"
        PYSPARK_TAG_SUFFIX="$(if [ "${WITH_PYSPARK}" = "true" ]; then echo _pyspark; fi)"
        TAG_NAME="${SELF_VERSION}_${SPARK_VERSION}_scala-${SCALA_VERSION}_hadoop-${HADOOP_VERSION}_python-${PYTHON_VERSION}${HIVE_TAG_SUFFIX}${PYSPARK_TAG_SUFFIX}_${DIST}"
        docker build ${DIST}/ -t "${IMAGE_NAME}:${TAG_NAME}" \
          --build-arg "SPARK_VERSION=${SPARK_VERSION}" \
          --build-arg "SCALA_VERSION=${SCALA_VERSION}" \
          --build-arg "HADOOP_VERSION=${HADOOP_VERSION}" \
          --build-arg "PYTHON_VERSION=${PYTHON_VERSION}" \
          --build-arg "AWS_JAVA_SDK_VERSION=${AWS_JAVA_SDK_VERSION}" \
          --build-arg "HIVE_TAG_SUFFIX=${HIVE_TAG_SUFFIX}" \
          --build-arg "PYSPARK_TAG_SUFFIX=${PYSPARK_TAG_SUFFIX}" \
          ;
    - name: Push Docker image
      run: bash push-images.sh
      env:
        DOCKER_USERNAME: ${{ secrets.DOCKER_USERNAME }}
        DOCKER_PASSWORD: ${{ secrets.DOCKER_PASSWORD }}
      if: github.event_name == 'push'

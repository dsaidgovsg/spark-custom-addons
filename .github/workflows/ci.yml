name: CI

on:
  push:
    branches:
    - master
    - v*
  pull_request:
    branches:
    - master
    - v*

jobs:
  build:
    strategy:
      matrix:
        version:
        - spark:        "2.4.4"
          scala:        "2.11"
          hadoop:       "2.7.3"
          python:       "3.5"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "alpine"
        - spark:        "2.4.4"
          scala:        "2.11"
          hadoop:       "2.7.3"
          python:       "3.6"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "alpine"
        - spark:        "2.4.4"
          scala:        "2.11"
          hadoop:       "2.7.3"
          python:       "3.7"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "alpine"
        - spark:        "2.4.4"
          scala:        "2.11"
          hadoop:       "2.7.3"
          python:       "3.8"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "alpine"
        - spark:        "2.4.4"
          scala:        "2.11"
          hadoop:       "3.1.0"
          python:       "3.5"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "alpine"
        - spark:        "2.4.4"
          scala:        "2.11"
          hadoop:       "3.1.0"
          python:       "3.6"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "alpine"
        - spark:        "2.4.4"
          scala:        "2.11"
          hadoop:       "3.1.0"
          python:       "3.7"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "alpine"
        - spark:        "2.4.4"
          scala:        "2.11"
          hadoop:       "3.1.0"
          python:       "3.8"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "alpine"
        - spark:        "2.4.4"
          scala:        "2.12"
          hadoop:       "2.7.3"
          python:       "3.5"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "alpine"
        - spark:        "2.4.4"
          scala:        "2.12"
          hadoop:       "2.7.3"
          python:       "3.6"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "alpine"
        - spark:        "2.4.4"
          scala:        "2.12"
          hadoop:       "2.7.3"
          python:       "3.7"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "alpine"
        - spark:        "2.4.4"
          scala:        "2.12"
          hadoop:       "2.7.3"
          python:       "3.8"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "alpine"
        - spark:        "2.4.4"
          scala:        "2.12"
          hadoop:       "3.1.0"
          python:       "3.5"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "alpine"
        - spark:        "2.4.4"
          scala:        "2.12"
          hadoop:       "3.1.0"
          python:       "3.6"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "alpine"
        - spark:        "2.4.4"
          scala:        "2.12"
          hadoop:       "3.1.0"
          python:       "3.7"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "alpine"
        - spark:        "2.4.4"
          scala:        "2.12"
          hadoop:       "3.1.0"
          python:       "3.8"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "alpine"
        - spark:        "2.4.5"
          scala:        "2.11"
          hadoop:       "2.7.3"
          python:       "3.5"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "alpine"
        - spark:        "2.4.5"
          scala:        "2.11"
          hadoop:       "2.7.3"
          python:       "3.6"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "alpine"
        - spark:        "2.4.5"
          scala:        "2.11"
          hadoop:       "2.7.3"
          python:       "3.7"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "alpine"
        - spark:        "2.4.5"
          scala:        "2.11"
          hadoop:       "2.7.3"
          python:       "3.8"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "alpine"
        - spark:        "2.4.5"
          scala:        "2.11"
          hadoop:       "3.1.0"
          python:       "3.5"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "alpine"
        - spark:        "2.4.5"
          scala:        "2.11"
          hadoop:       "3.1.0"
          python:       "3.6"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "alpine"
        - spark:        "2.4.5"
          scala:        "2.11"
          hadoop:       "3.1.0"
          python:       "3.7"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "alpine"
        - spark:        "2.4.5"
          scala:        "2.11"
          hadoop:       "3.1.0"
          python:       "3.8"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "alpine"
        - spark:        "2.4.5"
          scala:        "2.12"
          hadoop:       "2.7.3"
          python:       "3.5"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "alpine"
        - spark:        "2.4.5"
          scala:        "2.12"
          hadoop:       "2.7.3"
          python:       "3.6"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "alpine"
        - spark:        "2.4.5"
          scala:        "2.12"
          hadoop:       "2.7.3"
          python:       "3.7"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "alpine"
        - spark:        "2.4.5"
          scala:        "2.12"
          hadoop:       "2.7.3"
          python:       "3.8"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "alpine"
        - spark:        "2.4.5"
          scala:        "2.12"
          hadoop:       "3.1.0"
          python:       "3.5"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "alpine"
        - spark:        "2.4.5"
          scala:        "2.12"
          hadoop:       "3.1.0"
          python:       "3.6"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "alpine"
        - spark:        "2.4.5"
          scala:        "2.12"
          hadoop:       "3.1.0"
          python:       "3.7"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "alpine"
        - spark:        "2.4.5"
          scala:        "2.12"
          hadoop:       "3.1.0"
          python:       "3.8"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "alpine"
        - spark:        "2.4.4"
          scala:        "2.11"
          hadoop:       "2.7.3"
          python:       "3.5"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "debian"
        - spark:        "2.4.4"
          scala:        "2.11"
          hadoop:       "2.7.3"
          python:       "3.6"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "debian"
        - spark:        "2.4.4"
          scala:        "2.11"
          hadoop:       "2.7.3"
          python:       "3.7"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "debian"
        - spark:        "2.4.4"
          scala:        "2.11"
          hadoop:       "2.7.3"
          python:       "3.8"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "debian"
        - spark:        "2.4.4"
          scala:        "2.11"
          hadoop:       "3.1.0"
          python:       "3.5"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "debian"
        - spark:        "2.4.4"
          scala:        "2.11"
          hadoop:       "3.1.0"
          python:       "3.6"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "debian"
        - spark:        "2.4.4"
          scala:        "2.11"
          hadoop:       "3.1.0"
          python:       "3.7"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "debian"
        - spark:        "2.4.4"
          scala:        "2.11"
          hadoop:       "3.1.0"
          python:       "3.8"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "debian"
        - spark:        "2.4.4"
          scala:        "2.12"
          hadoop:       "2.7.3"
          python:       "3.5"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "debian"
        - spark:        "2.4.4"
          scala:        "2.12"
          hadoop:       "2.7.3"
          python:       "3.6"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "debian"
        - spark:        "2.4.4"
          scala:        "2.12"
          hadoop:       "2.7.3"
          python:       "3.7"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "debian"
        - spark:        "2.4.4"
          scala:        "2.12"
          hadoop:       "2.7.3"
          python:       "3.8"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "debian"
        - spark:        "2.4.4"
          scala:        "2.12"
          hadoop:       "3.1.0"
          python:       "3.5"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "debian"
        - spark:        "2.4.4"
          scala:        "2.12"
          hadoop:       "3.1.0"
          python:       "3.6"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "debian"
        - spark:        "2.4.4"
          scala:        "2.12"
          hadoop:       "3.1.0"
          python:       "3.7"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "debian"
        - spark:        "2.4.4"
          scala:        "2.12"
          hadoop:       "3.1.0"
          python:       "3.8"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "debian"
        - spark:        "2.4.5"
          scala:        "2.11"
          hadoop:       "2.7.3"
          python:       "3.5"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "debian"
        - spark:        "2.4.5"
          scala:        "2.11"
          hadoop:       "2.7.3"
          python:       "3.6"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "debian"
        - spark:        "2.4.5"
          scala:        "2.11"
          hadoop:       "2.7.3"
          python:       "3.7"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "debian"
        - spark:        "2.4.5"
          scala:        "2.11"
          hadoop:       "2.7.3"
          python:       "3.8"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "debian"
        - spark:        "2.4.5"
          scala:        "2.11"
          hadoop:       "3.1.0"
          python:       "3.5"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "debian"
        - spark:        "2.4.5"
          scala:        "2.11"
          hadoop:       "3.1.0"
          python:       "3.6"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "debian"
        - spark:        "2.4.5"
          scala:        "2.11"
          hadoop:       "3.1.0"
          python:       "3.7"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "debian"
        - spark:        "2.4.5"
          scala:        "2.11"
          hadoop:       "3.1.0"
          python:       "3.8"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "debian"
        - spark:        "2.4.5"
          scala:        "2.12"
          hadoop:       "2.7.3"
          python:       "3.5"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "debian"
        - spark:        "2.4.5"
          scala:        "2.12"
          hadoop:       "2.7.3"
          python:       "3.6"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "debian"
        - spark:        "2.4.5"
          scala:        "2.12"
          hadoop:       "2.7.3"
          python:       "3.7"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "debian"
        - spark:        "2.4.5"
          scala:        "2.12"
          hadoop:       "2.7.3"
          python:       "3.8"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "debian"
        - spark:        "2.4.5"
          scala:        "2.12"
          hadoop:       "3.1.0"
          python:       "3.5"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "debian"
        - spark:        "2.4.5"
          scala:        "2.12"
          hadoop:       "3.1.0"
          python:       "3.6"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "debian"
        - spark:        "2.4.5"
          scala:        "2.12"
          hadoop:       "3.1.0"
          python:       "3.7"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "debian"
        - spark:        "2.4.5"
          scala:        "2.12"
          hadoop:       "3.1.0"
          python:       "3.8"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "debian"
        - spark:        "2.3.4"
          scala:        "2.11"
          hadoop:       "2.7.3"
          python:       "3.5"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "alpine"
        - spark:        "2.3.4"
          scala:        "2.11"
          hadoop:       "2.7.3"
          python:       "3.6"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "alpine"
        - spark:        "2.3.4"
          scala:        "2.11"
          hadoop:       "2.7.3"
          python:       "3.7"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "alpine"
        - spark:        "2.3.4"
          scala:        "2.11"
          hadoop:       "2.7.3"
          python:       "3.8"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "alpine"
        - spark:        "2.3.4"
          scala:        "2.12"
          hadoop:       "2.7.3"
          python:       "3.5"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "alpine"
        - spark:        "2.3.4"
          scala:        "2.12"
          hadoop:       "2.7.3"
          python:       "3.6"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "alpine"
        - spark:        "2.3.4"
          scala:        "2.12"
          hadoop:       "2.7.3"
          python:       "3.7"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "alpine"
        - spark:        "2.3.4"
          scala:        "2.12"
          hadoop:       "2.7.3"
          python:       "3.8"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "alpine"
        - spark:        "2.3.4"
          scala:        "2.11"
          hadoop:       "2.7.3"
          python:       "3.5"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "debian"
        - spark:        "2.3.4"
          scala:        "2.11"
          hadoop:       "2.7.3"
          python:       "3.6"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "debian"
        - spark:        "2.3.4"
          scala:        "2.11"
          hadoop:       "2.7.3"
          python:       "3.7"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "debian"
        - spark:        "2.3.4"
          scala:        "2.11"
          hadoop:       "2.7.3"
          python:       "3.8"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "debian"
        - spark:        "2.3.4"
          scala:        "2.12"
          hadoop:       "2.7.3"
          python:       "3.5"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "debian"
        - spark:        "2.3.4"
          scala:        "2.12"
          hadoop:       "2.7.3"
          python:       "3.6"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "debian"
        - spark:        "2.3.4"
          scala:        "2.12"
          hadoop:       "2.7.3"
          python:       "3.7"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "debian"
        - spark:        "2.3.4"
          scala:        "2.12"
          hadoop:       "2.7.3"
          python:       "3.8"
          with_hive:    "true"
          with_pyspark: "true"
          dist:         "debian"
    runs-on: ubuntu-latest
    env:
      IMAGE_NAME: spark-custom-addons
      SELF_VERSION: "v2"
      BASE_VERSION: "v1"
      SPARK_VERSION: "${{ matrix.version.spark }}"
      SCALA_VERSION: "${{ matrix.version.scala }}"
      HADOOP_VERSION: "${{ matrix.version.hadoop }}"
      PYTHON_VERSION: "${{ matrix.version.python }}"
      WITH_HIVE: "${{ matrix.version.with_hive }}"
      WITH_PYSPARK: "${{ matrix.version.with_pyspark }}"
      DIST: "${{ matrix.version.dist }}"
    steps:
    - name: Checkout code
      uses: actions/checkout@v1
    - name: Install tera-cli
      run: |-
        wget https://github.com/guangie88/tera-cli/releases/download/v0.4.0/tera_linux_amd64 -O /tmp/tera
        chmod +x /tmp/tera
    - name: Check differences between ci.yml and ci.yml.tmpl
      run: |-
        cp .github/workflows/ci.yml .github/workflows/ci.yml.backup
        TERA=/tmp/tera ./templates/apply-vars.sh
        if ! diff .github/workflows/ci.yml .github/workflows/ci.yml.backup; then echo "ci.yml.tmpl and ci.yml differs!" && exit 1; fi
    - name: Shellcheck push image script
      run: shellcheck push-images.sh
    - name: Build Docker image
      run: |-
        HIVE_TAG_SUFFIX="$(if [ "${WITH_HIVE}" = "true" ]; then echo _hive; fi)"
        PYSPARK_TAG_SUFFIX="$(if [ "${WITH_PYSPARK}" = "true" ]; then echo _pyspark; fi)"
        TAG_NAME="${SELF_VERSION}_${SPARK_VERSION}_scala-${SCALA_VERSION}_hadoop-${HADOOP_VERSION}_python-${PYTHON_VERSION}${HIVE_TAG_SUFFIX}${PYSPARK_TAG_SUFFIX}_${DIST}"
        docker build "${DIST}/" -t "${IMAGE_NAME}:${TAG_NAME}" \
          --build-arg BASE_VERSION=${BASE_VERSION} \
          --build-arg SPARK_VERSION=${SPARK_VERSION} \
          --build-arg SCALA_VERSION=${SCALA_VERSION} \
          --build-arg HADOOP_VERSION=${HADOOP_VERSION} \
          --build-arg PYTHON_VERSION=${PYTHON_VERSION} \
          --build-arg HIVE_TAG_SUFFIX=${HIVE_TAG_SUFFIX} \
          --build-arg PYSPARK_TAG_SUFFIX=${PYSPARK_TAG_SUFFIX} \
    - name: Push Docker image
      run: |-
        export HIVE_TAG_SUFFIX="$(if [ "${WITH_HIVE}" = "true" ]; then echo _hive; fi)"
        export PYSPARK_TAG_SUFFIX="$(if [ "${WITH_PYSPARK}" = "true" ]; then echo _pyspark; fi)"
        bash push-images.sh
      env:
        DOCKER_USERNAME: ${{ secrets.DOCKER_USERNAME }}
        DOCKER_PASSWORD: ${{ secrets.DOCKER_PASSWORD }}
      if: github.event_name == 'push'
